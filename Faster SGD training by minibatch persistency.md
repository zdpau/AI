# Faster SGD training by minibatch persistency
https://arxiv.org/pdf/1806.07353.pdf
## Abstrsact
众所周知，对于大多数数据集来说，随机梯度下降（SGD）使用large-size minibatches通常会导致收敛速度慢和泛化能力差。 另一方面， large minibatches 具有很大的实际意义，因为它们可以更好地利用现代GPU。以前关于这个主题的文献主要集中在如何在使用large minibatches时调整主要的SGD参数（特别是学习率）。在这项工作中，我们引入了一个额外的特性，即我们称之为minibatch持久性，包括重复使用相同的minibatch进行K个连续的SGD迭代。 这里的计算猜想是一个大型的小批次包含了训练集的一个重要样本，所以可以稍微过度拟合它，而不会使泛化过分恶化。该方法旨在加速SGD收敛，并且还具有减少与内部GPU内存上的数据加载相关的开销的优势。我们在CIFAR-10上用AlexNet架构给出了计算结果，表明即使是小的持久性值（K = 2或5）也会导致显着更快的收敛，与标准的“一次性小批量”方法（K = 1）相比（或甚至更好）的推广，特别是当使用large minibatches时。吸取的教训是，小批次持久性可以成为处理大型小批次的简单而有效的方法。
## The idea
梯度下降是机器学习中的一种选择方法，特别是在深度学习中。 对于大型训练集，计算（可分解）损失函数的真实梯度然而非常昂贵，所以随机梯度下降（SGD）方法 - 在其在线或minibatches变体 - 在实践中是非常优选的，因为其优越的收敛速度和泛化程度。

近年来，并行体系结构（特别是GPU）的可用性表明使用更大和更大的minibatches，但是这可能是有问题的，因为已知minibatches越大，收敛性和泛化性越差; 参见例如[13,8,5,9]。 因此，寻找一种实际的方式来提供具有large minibatches计算机的大规模并行GPU是当前热门的研究课题。

最近有关该主题的研究指出，需要修改SGD参数（特别是学习率）以应对大型mb[13,8,5,9]，可能使用智能策略，如学习速率线性缩放和warm-up[2]。然而，所有出版的作品都理所当然地认为采用了“一次性小批量(disposable minibatch)”策略，即：在一个时期内，当前的小批量在每次SGD迭代时都会发生变化。

在本文中，我们研究了一种不同的策略，对于K（比如说）连续的SGD迭代重用相同的小mb，其中参数K称为小mb持久性（K = 1是标准规则）。 我们的直觉是大型mb包含大量关于训练集的信息，我们不想过早地放弃这些信息而浪费。 从某种意义上说，我们通过投资K-1额外的SGD迭代来“轻微过拟合”当前的小批次，然后用不同的小批次替换它。 该方法还具有降低与将新数据加载到GPU存储器的操作相关的计算开销的实际优点。

当然，坚持过多使用同一个小批次是一个过度拟合的风险策略，因此必须通过量化小批次持久性理念在一些已建立的环境中的优缺点来计算评估该方法的可行性。

在本文中，我们使用AlexNet [7]架构（缩减版本）报告了着名CIFAR-10 [6]数据集的计算测试结果，非常符合最近的工作精神[9]。 我们的研究结果表明，小的持久性值K = 2或5已经产生了显着改善的性能（在训练速度和泛化方面），其尺寸为256或更大的小型配件。 吸取的教训是，与minibatch持久性结合使用时，大型minibatches的使用变得更加吸引人，至少在我们考虑的设置中。

## Experiments
我们接下来会报告我们进行的一些测试的结果，以评估minibatch持续性对提高训练时间的实际影响，而不会以负面方式影响泛化。
### 1 setup
如前所述，我们的实验设置与[9]中的非常相似。 我们用CIFAR-10 [6]数据集解决了AlexNet [7]体系结构的简化版本，卷积层的步长等于1，核大小等于[11,5,3,3,3]，每通道数 层等于[64,192,384,256,256]，具有2x2内核和跨度2的最大池层，以及完全连接层的256个隐藏节点。 按照惯例，将数据集进行混洗，并将其划分为50,000个用于训练集的示例，剩余的10,000个用于测试集。

我们的优化方法是具有交叉熵损失动量[12]的SGD，这是softmax和负对数似然损失（NLLLoss）的组合。 对于所考虑的损失函数，大小为m的小批次的梯度计算为小批量中给定示例的m个梯度的总和。 更具体地说，令Li（θ）表示对于权重向量θ对小批量中第i个训练样例的损失函数的贡献。 对于大小为m的每个小批次，迭代t处的权重更新规则如下：**公式见原文**

培训使用PyTorch进行了100个时期[10]，学习率μ= 0.001，动量系数γ= 0.5。 对于所有（确定性）运行，初始随机种子（特别是影响随机权重初始化）保持相同。
所有的运行都在装有单GPU（Nvidia GTX1080 Ti）的个人电脑上进行; 报道的计算时间是wall-clock seconds。

我们的实验仅针对评估给定小批量大小为m的小批量持久性参数K的不同值的影响。 这就是我们决定使用上述非常基本的训练算法的原因。 特别是，批量标准化（batch normalization）[4]，ｄｒｏｐｏｕｔ[3]和数据增加在我们的实验中没有使用。 此外，动量系数和学习率是先验固定的（与小批量大小无关，并且不被视为调谐的超参数）并用于所有报道的实验中; 例如，参见[13]关于使用独立于小批量大小的学习速率的缺点的讨论。 因此，我们在100个时代（约60％）后达到的最终测试集精度绝对与现有技术水平无关。 当然，当使用更复杂的培训策略时，预计会有更好的结果 - 第3部分报告了这方面的一些初步结果。
### results
图1，图2和图3分别报告了我们针对小批量m = 32,256和512的实验结果。 请注意，根据[9]，CIFAR-10和（减少的）AlexNet架构的最佳性能是针对m≤8实现的，而256或512的小型配件被认为太大而不能在经典设置中产生有竞争力的结果 - 正如我们将看到的，当使用minibatch持久性时，这不再是真实的。

每个图绘制了小批次持久性参数K∈{1,2,5}的top-1精度和损失函数（均在测试集上），作为总计算时间（左侧的子图）和epoch数（右侧）的函数。 请注意，在一个历元中，每个训练样例都被评估了K次，因此可以预计每个历元的计算时间乘以K.这就是为什么要公平比较不同K值，明确报告计算时间非常重要。

图1涉及尺寸为m = 32的mb。较高的K值在epoch方面提供了改进的准确度（右侧的子图），但这并未转化为计算时间的改进（左侧的子图）。 实际上，用K = 5完成100个时期所花费的时间大约是情况K = 1（大约2000而不是700秒）的3倍，这在任何情况下都显着小于人们预期的5倍。另一方面，K = 5的运行在30个时期和约600秒后达到其最佳精度0.64450，而K = 1的运行仅在80个ｅｐｏｃｈ和同样６００秒时达到其最佳精度0.63640。至于损失，具有较大K的运行在时间和计算时间方面更早地开始过度拟合。 这种行为并不出乎意料，因为小批量太小而无法代表整个训练集，并且重复使用它几次就容易过度拟合。

当考虑具有m = 256个示例的medium-size mb时，整体情况发生根本变化（图2）。 在这里，过度拟合不太明显，并且完成100个epochs的计算时间没有随着K的值显着增加 - 因此证实当mb不小时，mb持续性在GPU利用方面具有积极效果。

当较大的mb开始运行时，mb持续性的积极影响更为明显（图3中的情况m = 512）。 在这里，K = 5的运行在准确性和损失方面都是明显的赢家。

上述结果相当令人鼓舞，并表明当与小批量持久性相结合时，使用大型mb变得更具吸引力。
## 3 Conclusions and future work
我们调查了在SGD培训期间连续K次重复使用相同的小批量的想法。计算实验表明，这个想法是有希望的，至少在我们考虑的特定环境中（AlexNet on CIFAR-10）。应该对不同的数据集和神经网络进行更广泛的测试，因为人们可能期望不同设置的不同（可能更糟）的行为。

在我们的测试中，为了比较，我们为所有K值固定了相同的学习率μ= 0.001。然而，我们也做了一些初步实验，采用了不同的政策，在重复使用相同的小批量时提高了学习率，即，等于k·μ的学习速率被应用于相同mb的第k次使用（k = 1，...，K）。

理性的是，每次重复使用小批量时，计算出的更新方向变得更可靠（至少，为了优化当前mb中的loss function）。 这种自适应学习率策略让人想起[11]中介绍的循环学习率培训政策(Cyclical Learning Rate training policy)，但它是根据我们的设置量身定制的。根据图4和图5中报告的初步结果，该方法看起来相当有效 - 对于小批量大小m = 256（未报告），观察到类似的行为。 因此，未来的工作应致力于更好地研究这种（或类似的）适应性策略。



