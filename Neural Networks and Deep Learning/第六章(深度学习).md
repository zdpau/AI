
主要就是对代码的解读，用的是theano框架。文件是network3.py，有空再看吧。

本章，我们将给出可以用来训练深度神经网络的技术，并在实战中应用它们。同样我们也会从更加广阔的视角来看神经网络，简要地回顾近期有关深度神经网络在图像识别、语音识别和其他应用中的研究进展。然后，还会给出一些关于未来神经网络又或人工智能的简短的推测性的看法。

本章主要的部分是对最为流行神经网络之一的深度卷积网络的介绍。我们将细致地分析一个使用卷积网络来解决 MNIST 数据集的手写数字识别的例子.我们将从浅层的神经网络开始来解决上面的问题。通过多次的迭代，我们会构建越来越强大的网络。在这个过程中，也将要探究若干强大技术：卷积、pooling、使用GPU来更好地训练、训练数据的算法性扩展（避免过匹配）、dropout 技术的使用（同样为了防止过匹配现象）、网络的 ensemble 使用 和 其他技术。最终的结果能够接近人类的表现。

本章剩下的部分，我们将会从一个更加宽泛和宏观的角度来讨论深度学习。概述一些神经网络的其他模型，例如 RNN 和 LSTM 网络，以及这些网络如何在语音识别、自然语言处理和其他领域中应用的。最后会试着推测一下，神经网络和深度学习未来发展的方向，会从 intention-driven user interfaces 谈谈深度学习在人工智能的角色。

## Recurrent Neural Networks (RNNs)：

在前馈神经网络中，单独的输入完全确定了剩下的层上的神经元的激活值。可以想象，这是一幅静态的图景：网络中的所有事物都被固定了，处于一种“冰冻结晶”的状态。但假如，我们允许网络中的元素能够以动态方式不断地变化。例如，隐藏神经元的行为不是完全由前一层的隐藏神经元，而是同样受制于更早的层上的神经元的激活值。这样肯定会带来跟前馈神经网络不同的效果。也可能隐藏和输出层的神经元的激活值不会单单由当前的网络输入决定，而且包含了前面的输入的影响。

拥有之类时间相关行为特性的神经网络就是递归神经网络，常写作 RNN。更加一般的想法是，**RNN 是某种体现出了随时间动态变化的特性的神经网络。**也毫不奇怪，RNN 在处理时序数据和过程上效果特别不错。这样的数据和过程正是语音识别和自然语言处理中常见的研究对象。

## Long Short-term Memory units(LSTMs)：
影响 RNN 的一个挑战是前期的模型会很难训练，甚至比前馈神经网络更难。原因就是我们在上一章提到的不稳定梯度的问题。回想一下，这个问题的通常表现就是在反向传播的时候梯度越变越小。这就使得前期的层学习非常缓慢。在 RNN 中这个问题更加糟糕，因为梯度不仅仅通过层反向传播，还会根据时间进行反向传播。如果网络运行了一段很长的时间，就会使得梯度特别不稳定，学不到东西。幸运的是，可以引入一个成为 long short-term memory 的单元进入 RNN 中。LSTM 最早是由 Hochreiter 和 Schmidhuber 在 1997 年提出，就是为了解决这个不稳定梯度的问题。LSTM 让 RNN 训练变得相当简单，很多近期的论文（包括我在上面给出的那些）都是用了 LSTM 或者相关的想法。

## 深度信念网络，生成式模型和 Boltzmann 机：对深度学习的兴趣产生于 2006 年，最早的论文就是解释如何训练称为 深度信念网络 （DBN）的网络。

* 参见 Geoffrey Hinton, Simon Osindero 和 Yee-Whye Teh 在 2006 年的A fast learning algorithm for deep belief nets , 及 Geoffrey Hinton 和 Ruslan Salakhutdinov 在2006 年的相关工作Reducing the dimensionality of data with neural networks

但近些年前馈网络和 RNN 的流行，盖过了 DBN 的风头。尽管如此，DBN 还是有几个有趣的特性。 一个就是 DBN 是一种生成式模型。在前馈网络中，我们指定了输入的激活函数，然后这些激活函数便决定了网络中后面的激活值。而像 DBN 这样的生成式模型可以类似这样使用，但是更加有用的可能就是指定某些特征神经元的值，然后进行“反向运行”，产生输入激活的值。具体讲，DBN 在手写数字图像上的训练同样可以用来生成和手写数字很像的图像。换句话说，DBN 可以学习写字的能力。所以，生成式模型更像人类的大脑：不仅可以读数字，还能够写出数字。用 Geoffrey Hinton 本人的话就是：“要识别对象的形状，先学会生成图像。” （to recognize shapes，first learn to generate images） 另一个是 DBN 可以进行无监督和半监督的学习。例如，在使用 图像数据学习时，DBN 可以学会有用的特征来理解其他的图像，即使，训练图像是无标记的。这种进行非监督学习的能力对于根本性的科学理由和实用价值（如果完成的足够好的话）来说都是极其有趣的。

## 神经网络的未来
意图驱动的用户接口：有个很古老的笑话是这么说的：“一位不耐烦的教授对一个困惑的学生说道，‘不要光听我说了什么，要听懂我说的含义。’”。历史上，计算机通常是扮演了笑话中困惑的学生这样的角色，对用户表示的完全不知晓。而现在这个场景发生了变化。我仍然记得自己在 Google 搜索的打错了一个查询，搜索引擎告诉了我“你是否要的是[这个正确的查询]?”，然后给出了对应的搜索结果。Google 的 CEO Larry Page 曾经描述了最优搜索引擎就是准确理解用户查询的含义，并给出对应的结果。

**这就是意图驱动的用户接口的愿景。在这个场景中，不是直接对用户的查询词进行结果的反馈，搜索引擎使用机器学习技术对大量的用户输入数据进行分析，研究查询本身的含义，并通过这些发现来进行合理的动作以提供最优的搜索结果。**

而意图驱动接口这样的概念也不仅仅用在搜索上。在接下来的数十年，数以千计的公司会将产品建立在机器学习来设计满足更高的准确率的用户接口上，准确地把握用户的意图。现在我们也看到了一些早期的例子：如苹果的Siri；Wolfram Alpha；IBM 的 Watson；可以对照片和视频进行注解的系统；还有更多的。

大多数这类产品会失败。启发式用户接口设计非常困难，我期望有更多的公司会使用强大的机器学习技术来构建无聊的用户接口。最优的机器学习并不会在你自己的用户接口设计很糟糕时发挥出作用。但是肯定也会有能够胜出的产品。随着时间推移，人类与计算机的关系也会发生重大的改变。不久以前，比如说，2005 年——用户从计算机那里得到的是准确度。因此，很大程度上计算机很古板的；一个小小的分号放错便会完全改变和计算机的交互含义。但是在以后数十年内，我们期待着创造出意图驱动的用户借款购，这也会显著地改变我们在与计算机交互的期望体验。

**我相信，深度学习会继续发展。学习概念的层次特性、构建多层抽象的能力，看起来能够从根本上解释世界。**

在物理学、数学、化学等等领域都存在这样的情况。这些领域开始时显现出一整块的知识，只有一点点深刻的观点。早期的专家可以掌握所有的知识。但随着时间流逝，这种一整块的特性就发生的演变。我们发现很多深刻的新想法，对任何一个人来说都是太多以至于难以掌握所有的想法。所以，这个领域的社会结构就开始重新组织，围绕着这些想法分离。我们现在看到的就是领域被不断地细分，子领域按照一种复杂的、递归的、自指的社会结构进行分解，而这些组织关系也反映了最深刻的那些想法之间的联系。**因此，知识结构形成了科学的社会组织关系。但这些社会关系反过来也会限制和帮助决定那些可以发现的事物。**这就是 Conway 法则在科学上变体版本。 那么，这又会对深度学习或者 AI 有什么影响呢？

因为在 AI 发展早期，存在对它的争论，一方认为，“这并不是很难的一件事，我们已经有[超级武器]了。”，反对方认为，“超级武器并不足够”。深度学习就是最新的超级武器，更早的有逻辑、Prolog或者专家系统，或者当时最牛的技术。这些论点的问题就是他们并没有以较好的方式告诉你这些给定的候选超级武器如何强大。当然，我们已经花了一章来回顾深度学习可以解决具备相当挑战性的问题的证据。看起来令人兴奋，前景光明。但是那些像 Prolog 或者 Eurisko 或者专家系统在它们的年代也同样如此。所以，那些观点或者方法看起来很有前景并没有什么用。我们如何区分出深度学习和早期的方法的本质差异呢？Conway 法则给出了一个粗略和启发性的度量，也就是评价和这些方法相关的社会关系的复杂性。

所以，这就带来了两个需要回答的问题。第一，根据这种社会复杂性度量，方法集和深度学习关联的强度是怎么样的？第二，我们需要多么强大的理论来构建一个通用的人工智能？

对第一个问题：我们现在看深度学习，这是一个激情澎湃却又**相对单一的领域**。有一些深刻的想法，一些主要的学术会议，其中若干会议之间也存在着很多重叠。然后，**一篇篇的论文在不断地提升和完善同样的一些基本想法**：使用 SGD（或者类似的变体）来优化一个代价函数。这些想法非常成功。但是我们现在**还没有看到子领域的健康发展**，每个人在研究自己的深刻想法，将深度学习推向很多的方向。所以，根据社会复杂性度量，忽略文字游戏，深度学习仍然是一个相当粗浅的领域。现在还是可以完全地掌握该领域大多数的深刻想法的。

第二个问题：一个想法集合需要如何复杂和强大才能达到 AI？当然，对这个问题的答案是：**无人知晓**。但在附录部分，我讨论了一些已有的观点。我比较乐观地认为，将会使用很多很多深刻的观点来构建 AI。所以，Conway 法则告诉我们，为了达到这样的目标，我们必需看到很多交叉关联的学科，以一种复杂和可能会很奇特的结构的出现，这种结构也映射出了那些最深刻洞察之间的关系。目前在使用神经网络和深度学习中，这样的社会结构还没有出现。并且，我也坚信离真正使用深度学习来发展通用 AI 还有至少几十年的距离。 催生这个可能看起来很易见的试探性的并不确定的论断已经带给我很多的困扰。毫无疑问，这将会让那些寄望于获得确定性的人们变得沮丧。读了很多网络上的观点，我发现很多人在大声地下结论，对 AI 持有非常乐观的态度，但很多是缺少确凿证据和站不住脚的推断的。我很坦白的观点是：现在下这样乐观的结论还为之过早。正如一个笑话所讲，如果你问一个科学家，某个发现还有多久才会出现，他们会说 10 年（或者更多），其实真正的含义就是“我不知道”。AI，像受控核聚变和其他技术一样，已经发展远超 10 年已经 60 多年了。另一方面，我们在深度学习中确确实实在做的其实就是还没有发现极限的强大技术，还有哪些相当开放的根本性问题。这是令人兴奋异常的创造新事物的机遇。
