当一个高尔夫球员刚开始学习打高尔夫时，他们通常会在挥杆的练习上花费大多数时间。慢慢地他们才会在基本的挥杆上通过变化发展其他的击球方式，学习低飞球、左曲球和右曲球。类似的，我们现在仍然聚焦在反向传播算法的理解上。这就是我们的“基本挥杆”——神经网络中大部分工作学习和研究的基础。本章，我会解释若干技术能够用来提升我们关于反向传播的初级的实现，最终改进网络学习的方式。

本章涉及的技术包括：更好的代价函数的选择——交叉熵 代价函数；四中规范化方法（L1 和 L2 规范化，dropout 和训练数据的人工扩展），这会让我们的网络在训练集之外的数据上更好地泛化；更好的权重初始化方法；还有帮助选择好的超参数的启发式想法。

## 交叉熵代价函数
将交叉熵看做是代价函数有两点原因。第一，它使非负的，C>0。可以看出：(a) 公式(57)的和式中所有独立的项都是非负的，因为对数函数的定义域是 (0,1)；(b) 前面有一个负号。

第二，如果神经元实际的输出接近目标值。假设在这个例子中，y=0 而 a≈0。这是我们想到得到的结果。我们看到公式(57)中第一个项就消去了，因为 y=0，而第二项实际上就是 −ln(1−a)≈0。反之，y=1 而 a≈1。所以在实际输出和目标输出之间的差距越小，最终的交叉熵的值就越低了。

综上所述，交叉熵是非负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是二次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有一个比二次代价函数更好的特性就是它避免了学习速度下降的问题。

它告诉我们权重学习的速度受到 σ(z)−y，也就是输出中的误差的控制。更大的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在二次代价函数中类似公式中 σ′(z) 导致的学习缓慢，见公式(55)。当我们使用交叉熵的时候，σ′(z) 被约掉了，所以我们不再需要关心它是不是变得很小。这种约除就是交叉熵带来的特效。

如果你观测的足够仔细，你可以发现代价函数曲线要比二次代价函数训练前面部分要陡很多。正是交叉熵带来的快速下降的坡度让神经元在处于误差很大的情况下能够逃脱出学习缓慢的困境，这才是我们直觉上所期待的效果。

**那么我们应该在什么时候用交叉熵来替换二次代价函数？**实际上，如果在输出神经元使用 sigmoid 激活函数时，交叉熵一般都是更好的选择。为什么？考虑一下我们初始化网络的时候通常使用某种随机方法。可能会发生这样的情况，这些初始选择会对某些训练输入误差相当明显——比如说，目标输出是 1，而实际值是 0，或者完全反过来。如果我们使用二次代价函数，那么这就会导致学习速度的下降。它并不会完全终止学习的过程，因为这些权重会持续从其他的样本中进行学习，但是显然这不是我们想要的效果。

## 理解神经元饱和和如何解决这个问题

**问题**：已经深入讨论了使用二次代价函数的网络中在输出神经元饱和时候学习缓慢的问题，另一个可能会影响学习的因素就是在方程(61)中的 xj 项。由于此项，当输入 xj 接近 0 时，对应的权重 xj 会学习得相当缓慢。解释为何不可以通过改变代价函数来消除 xj 项的影响。

## 交叉熵的含义？源自哪里？
我们对于交叉熵的讨论聚焦在代数分析和代码实现。这虽然很有用，但是也留下了一个未能回答的更加宽泛的概念上的问题，如：交叉熵究竟表示什么？存在一些直觉上的思考交叉熵的方法么？我们如何想到这个概念？

让我们从最后一个问题开始回答：什么能够激发我们想到交叉熵？假设我们发现学习速度下降了，并理解其原因是因为在公式(55)(56)中的 σ′(z) 那一项。在研究了这些公式后，我们可能就会想到选择一个不包含 σ′(z) 的代价函数。

## Softmax
softmax 的想法其实就是为神经网络定义一种新式的输出层。softmax 层的输出是一些相加为 1 正数的集合。换言之，softmax 层的输出可以被看做是一个概率分布。暴力的话，直接将最大的数设为1。在神经网络中，描述多类分类问题时，输出层会设置多个节点，常用 softmax 作为输出层的激活函数，称为softmax层.

softmax的指数函数确保了所有的输出激活值是正数。然后分母的求和又保证了 softmax 的输出和为 1。所以这个特定的形式不再像之前那样难以理解了：反而是一种确保输出激活值形成一个概率分布的自然的方式。你可以将其想象成一种重新调节 zLj 的方法，然后将这个结果整合起来构成一个概率分布。

https://baike.baidu.com/item/Softmax%E5%87%BD%E6%95%B0/22772270?fr=aladdin (最底下有sigmoid和softmax的对比)

这样的效果很令人满意。在很多问题中，将这些激活值作为网络对于某个输出正确的概率的估计非常方便。所以，比如在 MNIST 分类问题中，我们可以将 aLj 解释成网络估计正确数字分类为 j 的概率。

对比一下，如果输出层是 sigmoid 层，那么我们肯定不能假设激活值形成了一个概率分布。我不会证明这一点，但是源自 sigmoid 层的激活值是不能够形成一种概率分布的一般形式的。所以使用 sigmoid 输出层，我们没有关于输出的激活值的简单的解释。

## 学习缓慢问题：我们现在已经对 softmax 神经元有了一定的认识。但是我们还没有看到 softmax 会怎么样解决学习缓慢问题。

为了理解这点，先定义一个 log-likelihood 代价函数。我们使用 x 表示训练输入，y 表示对应的目标输出。然后关联这个训练输入样本的 log-likelihood 代价函数就是： C≡−lnaLy.

如果我们训练的是 MNIST 图像，输入为 7 的图像，那么对应的 log-likelihood 代价就是 −lnaL7。看看这个直觉上的含义，想想当网络表现很好的时候，也就是确认输入为 7 的时候。这时，他会估计一个对应的概率 aL7 跟 1 非常接近，所以代价 −lnaL7 就会很小。反之，如果网络的表现糟糕时，概率 aL7 就变得很小，代价 −lnaL7 随之增大。所以 log-likelihood 代价函数也是满足我们期待的代价函数的条件的。

关于学习缓慢问题呢？为了分析它，回想一下学习缓慢的关键就是量 ∂C/∂wLjk 和 ∂C/∂bLj 的变化情况。softmax的情况是：

**公式81,82**  

将 softmax 输出层和 log-likelihood 组合  对照   sigmoid 输出层和交叉熵的组合类比着看是非常有用的。

有了这样的相似性，你会使用哪一种呢？实际上，在很多应用场景中，这两种方式的效果都不错。作为一种更加通用的视角，**softmax 加上 log-likelihood 的组合更加适用于那些需要将输出激活值解释为概率的场景。当然这不总是合理的，但是在诸如 MNIST 这种有着不重叠的分类问题上确实很有用。**

## 过拟合和规范化
