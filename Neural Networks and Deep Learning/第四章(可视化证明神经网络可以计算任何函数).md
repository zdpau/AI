https://www.jianshu.com/p/1d80023119cc （不全）

https://www.cnblogs.com/yeluzi/p/7491619.html （看这个）

http://neuralnetworksanddeeplearning.com/chap4.html

可视化证明，没怎么看完，到时候再看吧。

神经网络的一个最引人注目的特点就是它实际上可以计算任何的函数。表明神经网络拥有一种普遍性（universality）。不管拿过来什么函数，我们都确信存在一个神经网络可以计算它。

在解释为何普遍性定理成立前，我想要说说关于不大形式化的表述“神经网络可以计算任何函数”的两个提醒。
>* 第一点，这句话不是说一个网络可以被用来准确地计算任何函数。而是说，我们可以获得尽可能好的一个近似。通过增加隐藏元的数量，我们可以提升近似的精度。
>* 第二点，就是可以按照上面的方式近似的函数类其实是连续函数。如果函数不是连续的，也就是会有突然、极陡的跳跃，那么一般来说无法使用一个神经网络进行近似。

总结一下，更加准确的关于普遍性定理的表述是包含一个隐藏层的神经网络可以被用来按照任意给定的精度来近似任何连续函数。

原文里有图可以调整W和偏差b，来看图形的变化。总结就是：
>* 向左拉拽来降低偏差。这时可以看到在偏差下降的时候，这个图向右移动了，但是它的形状仍然没有发生变化; 向右拉拽来增加这个值。你会看到在偏差增加时，图向左侧移动了，不过它的形状没有改变。
>* 看到在降低权重的时候，这个曲线变得宽平了。你可能需要同时改变偏差，这样可以保证曲线在展示的范围内; 增加权重时，曲线变得很陡，类似根号。

