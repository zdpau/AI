https://blog.csdn.net/qq_31192383/article/details/77429409 

https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html

反向传播说白了根据根据J（损失函数)的公式对W和b求偏导，也就是求梯度。因为我们需要用梯度下降法来对参数进行更新，而更新就需要梯度。

**目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。**这样做是有一定原因的，首先，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数 f(x) f(x)f(x)(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数。


1，通过第一章知道了，神经网络如何通过梯度下降算法学习，从而改变权重和偏差。但是，前面我们并没有讨论如何计算代价函数的梯度，这是一个很大的遗憾。这一篇文章，我们将介绍一种称为反向传播的快速计算梯度的算法。

2, 反向传播算法的核心是一个对于任何权重或者偏差计算其关于代价函数的偏导数的表达式。这个表达式告诉我们改变权重和偏差时，代价函数的变化快慢。

反向传播算法是神经网络中最有效的算法，其主要的思想是将网络最后输出的结果计算其误差，并且将误差反向逐级传下去。 方向传播运用的是链式求导的基本思想（隐函数求导）

前向传递输入信号直至输出产生误差，反向传播误差信息更新权重矩阵。

梯度下降可以应对带有明确求导函数的情况，或者说可以应对那些可以求出误差的情况，比如逻辑回归（Logistic Regression），我们可以把它看做没有隐层的网络；但对于多隐层的神经网络，输出层可以直接求出误差来更新参数，但其中隐层的误差是不存在的，因此不能对它直接应用梯度下降，而是先将误差反向传播至隐层，然后再应用梯度下降，其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。


3，代价函数可以工作的两个假设：首先我们需要假设代价函数可以写成对于每一个训练样本x的代价值Cx的均值。

我们需要这个假设的原因是，反向传播实际上是计算单个训练样本的偏导数值∂Cx/∂w和∂Cx/∂b。然后我们通过在所有样本上进行平均后才得到了∂C/∂w和∂C/∂b。（反向传播是关于理解如何改变权值和偏差从而改变代价函数。最终的意义在于计算偏导数∂C/∂wljk和∂C/∂blj。）

第二个假设是代价可以写成神经网络输出的函数。

4，我们使用δl表示关联l层的误差向量。反向传播将会提供计算每一层误差δl的方法，然后将这些误差和我们真正关心的∂C/∂wjkl和∂C/∂bjl关联起来。


5，反向传播的四个基本方程（这些方程给了我们计算误差δl和代价函数梯度的方法(别忘了我们的目的是计算代价函数关于权重和偏差的梯度））
![avatar](https://img-blog.csdn.net/20170829160050372?/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzExOTIzODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

6, 给定小批量样本大小为m，则随机梯度下降算法为:
![avatar](https://img-blog.csdn.net/20170829162147969?/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzExOTIzODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
