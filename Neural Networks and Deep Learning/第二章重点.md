https://blog.csdn.net/qq_31192383/article/details/77429409 

https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter2.html

1，通过第一章知道了，神经网络如何通过梯度下降算法学习，从而改变权重和偏差。但是，前面我们并没有讨论如何计算代价函数的梯度，这是一个很大的遗憾。这一篇文章，我们将介绍一种称为反向传播的快速计算梯度的算法。

2, 反向传播算法的核心是一个对于任何权重或者偏差计算其关于代价函数的偏导数的表达式。这个表达式告诉我们改变权重和偏差时，代价函数的变化快慢。

反向传播算法是神经网络中最有效的算法，其主要的思想是将网络最后输出的结果计算其误差，并且将误差反向逐级传下去。 方向传播运用的是链式求导的基本思想（隐函数求导）

3，代价函数可以工作的两个假设：首先我们需要假设代价函数可以写成对于每一个训练样本x的代价值Cx的均值。

我们需要这个假设的原因是，反向传播实际上是计算单个训练样本的偏导数值∂Cx/∂w和∂Cx/∂b。然后我们通过在所有样本上进行平均后才得到了∂C/∂w和∂C/∂b。（反向传播是关于理解如何改变权值和偏差从而改变代价函数。最终的意义在于计算偏导数∂C/∂wljk和∂C/∂blj。）

第二个假设是代价可以写成神经网络输出的函数。

4，我们使用δl表示关联l层的误差向量。反向传播将会提供计算每一层误差δl的方法，然后将这些误差和我们真正关心的∂C/∂wjkl和∂C/∂bjl关联起来。

5，反向传播的四个基本方程（这些方程给了我们计算误差δl和代价函数梯度的方法(别忘了我们的目的是计算代价函数关于权重和偏差的梯度））
![avatar](https://img-blog.csdn.net/20170829160050372?/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzExOTIzODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

6, 给定小批量样本大小为m，则随机梯度下降算法为:
![avatar](https://img-blog.csdn.net/20170829162147969?/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzExOTIzODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
