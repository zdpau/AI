http://neuralnetworksanddeeplearning.com/ 

https://tigerneil.gitbooks.io/neural-networks-and-deep-learning-zh/content/chapter1.html （中文）


http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons

https://blog.csdn.net/qq_31192383/article/details/77145993 （有人翻译了）

http://www.cnblogs.com/pathrough/p/5855084.html （上面的最后不全，接这一个）

1，感知机----→ 

2，实现NAND----→

3，sigmoid神经元（sigmoid函数也拥有输入向量，但是它的输入向量不再仅限于0和1，而是0到1之间的连续值。比如，0.1314可以作为sigmoid神经元的输入值。同样，sigmoid神经元对每个输入都有分配权重和一个总的偏差。但是输出也不再是0和1，而是σ(w⋅x)+b，其中σ被称为sigmoid函数。

----→

4，神经网络（假设我们试图识别一幅图像是否是9，一个自然的方法就是将该图片的灰度值编码作为神经元的输入。如果这个图片是64X64的灰度图，那么我们的输入神经元就有64X64=4096个输入神经元，它的值随着灰度在0到1里适当的变化。输出神经元只有一个，输出的值小于0.5表示这个数字不是9，反之就是9.

----→

5，一个简单的手写数字分类网络（我们的训练数据有许多28X28的手写数字的像素图组成，也就是说我们的输入层包含了28X28=784个神经元，一个神经元对应一个像素位的值。输入的像素值为灰度值，0.0表示白色，1.0表示黑色，0到1之间的值表示不同程度的灰色。

网络的第二层为隐藏层，我们记隐藏层的神经元数量为n，我们将对n的取值进行实验。这个图例中展示了一个小的隐藏层，它的隐藏层只有15个。

网络的输出层包含了10个神经元。如果第一个神经元被激活，也就是输出值无限接近1，那么我们可以认为这个网络识别出来的数字是0.如果是第二个神经元激活，那么识别出来的数字为1。更准确的说，我们将对输出神经元从0到9编号，然后找出激活值最高的神经元。如果编号为6的神经元激活值最高，那么我们认为输入的数字为6。

让我们把目光放在第一个输出神经上，该神经决定了是否该数字是0。它是通过权衡隐藏层所输出的信息做出判断的。那么隐藏层的神经元做了些什么呢？假设隐藏层的第一个神经元是为了检测图像中是否存在如下图形，如果隐藏层的这四个神经元都被激活，那么我们可以认为这个图像上的数字是0。）

----→

6，使用梯度下降算法进行学习：我们用x表示一个训练输入。显然x是一个28X28=784的向量，向量中每一个元素表示图像中的一个灰度值。我们用y=y(x)表示对应的期望输出值，其中y是一个10维的向量。比如，有一个特定的显示为6的图像输入为x，那么它期望的输出值应该为y(x)=(0,0,0,0,0,0,1,0,0,0)T，其中T表示矩阵的转置。

我们想有这样一个算法，它可以让我们找到权重和偏差，这样网络的输出y(x)可以拟合所有的输入x。为了量化我们如何实现这个目标，我们定义一个代价函数


----→

7，实现我们的数字分类网络（代码部分）
我的理解：
* 1，先构造一个3层神经网络，初始化权重，偏差，
* 2，net.weights[1]是一个存储着链接第二层和第三层神经元权重矩阵。由于net.weights[1]写起来很冗长，我们就用w表示这个矩阵。也就是wjk表示的是第二次的第k个神经元和第三层的第j个神经元之间的权重。那么第三层神经元的激活向量为：a' = σ(w⋅x)+b (a是第二层神经元的激活向量(输出向量)。为了获得a‘，我们用a乘以权重矩阵w,然后再加上偏差向量b。然后对向量wa+b中的每个元素使用σ函数(sigmoid函数)。)
* 3，feedforward方法：它对于网络给定输入a,返回对应的输出，这个方法是对每一层应用方程：a = sigmoid(np.dot(w, a)+b)
* 4, SGD:先输入training_data 等，然后update_mini_batch（**这里用到反向传播的算法，可以快速的计算代价函数的梯度。因此update_mini_batch 的⼯作仅仅是对 mini_batch 中的每⼀个训练样本计算梯度，然后适当地更新 self.weights 和 self.biases。**）


