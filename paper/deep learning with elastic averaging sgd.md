## Abstract
We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.

我们研究了通信约束下并行计算环境中深度学习的随机优化问题。在该设置中提出了一种新算法，其中并发过程（本地工作者）之间的工作的通信和协调基于弹性力，该弹性力将它们计算的参数与由参数服务器（主）存储的中心变量链接。该算法使本地工作人员能够进行更多探索，即该算法通过减少本地工作人员与主人之间的通信量，允许本地变量从中心变量进一步波动。我们凭经验证明，在深度学习环境中，由于存在许多局部最优，允许更多的探索可以导致改进的性能。我们提出了新算法的同步和异步变体。我们在循环方案中提供异步变量的稳定性分析，并将其与更常见的并行方法ADMM进行比较。我们表明，当满足简单的稳定条件时，EASGD的稳定性得到保证，而ADMM则不然。我们还提出了基于动量的算法版本，可以在同步和异步设置中应用。该算法的异步变体应用于训练卷积神经网络，用于CIFAR和ImageNet数据集上的图像分类。实验表明，与DOWNPOUR和其他常见的基线方法相比，新算法加速了深层架构的训练，并且还具有非常高的通信效率。

## Introduction
One of the most challenging problems in large-scale machine learning is how to parallelize the training of large models that use a form of stochastic gradient descent (SGD) [1]. There have been attempts to parallelize SGD-based training for large-scale deep learning models on large number of CPUs, including the Google’s Distbelief system [2]. But practical image recognition systems consist of large-scale convolutional neural networks trained on few GPU cards sitting in a single computer [3, 4]. The main challenge is to devise parallel SGD algorithms to train large-scale deep learning models that yield a significant speedup when run on multiple GPU cards.

大规模机器学习中最具挑战性的问题之一是如何并行化使用随机梯度下降（SGD）形式的大型模型的训练[1]。已经尝试在大量CPU上并行化基于SGD的大规模深度学习模型训练，包括谷歌的Distbelief系统[2]。但实际的图像识别系统包括大规模的卷积神经网络，这些神经网络只需要坐在一台计算机上的少量GPU卡上进行训练[3,4]。主要的挑战是设计并行的SGD算法来训练大规模深度学习模型，这些模型在多个GPU卡上运行时可以产生显着的加速。

In this paper we introduce the Elastic Averaging SGD method (EASGD) and its variants. EASGD is motivated by quadratic penalty method [5], but is re-interpreted as a parallelized extension of the averaging SGD algorithm [6]. The basic idea is to let each worker maintain its own local parameter, and the communication and coordination of work among the local workers is based on an elastic force which links the parameters they compute with a center variable stored by the master. The center variable is updated as a moving average where the average is taken in time and also in space over the parameters computed by local workers. 

在本文中，我们介绍了弹性平均SGD方法（EASGD）及其变体。 EASGD受二次惩罚法[5]的推动，但被重新解释为平均SGD算法的并行扩展[6]。基本思想是让每个工人保持自己的本地参数，并且本地工人之间的工作的沟通和协调基于弹性力，该弹性力将他们计算的参数与由主人存储的中心变量相关联。中心变量更新为移动平均值，其中平均值在时间上以及在本地工作人员计算的参数的空间中。

The main contribution of this paper is a new algorithm that provides fast convergent minimization while outperforming DOWNPOUR method [2] and other baseline approaches in practice. Simultaneously it reduces the communication overhead between the master and the local workers while at the same time it maintains high-quality performance measured by the test error. The new algorithm applies to deep learning settings such as parallelized training of convolutional neural networks.

本文的主要贡献是提供快速收敛最小化的新算法，同时在实践中优于DOWNPOUR方法[2]和其他基线方法。同时，它减少了主设备和本地工作人员之间的通信开销，同时保持了由测试错误测量的高质量性能。新算法适用于深度学习设置，如卷积神经网络的并行训练。

The article is organized as follows. Section 2 explains the problem setting, Section 3 presents the synchronous EASGD algorithm and its asynchronous and momentum-based variants, Section 4 provides stability analysis of EASGD and ADMM in the round-robin scheme, Section 5 shows experimental results and Section 6 concludes. The Supplement contains additional material including additional theoretical analysis.

文章的结构安排如下。第2节解释了问题设置，第3节介绍了同步EASGD算法及其异步和基于动量的变量，第4节提供了循环方案中EASGD和ADMM的稳定性分析，第5节显示了实验结果，第6节得出结论。补充材料包含其他材料，包括额外的理论分析。

## Problem setting
The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem.The quadratic penalty term ρ in Equation 2 is expected to ensure that local workers will not fall into different attractors that are far away from the center variable. This paper focuses on the problem of reducing the parameter communication overhead between the master and local workers [10, 2, 11, 12, 13]. The problem of data communication when the data is distributed among the workers [7, 14] is a more general problem and is not addressed in this work. We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [15].

这里还有些内容，数学公式。
这两个目标的等价问题在文献中被研究并被称为可扩充性或全局变量共识问题。方程2中的二次惩罚项ρ有望确保本地工人不会陷入不同的吸引子。 远离中心变量。 本文重点讨论减少主工人和本地工人之间参数通信开销的问题[10,2,11,12,13]。 当数据在工作人员之间分配时数据通信的问题[7,14]是一个更普遍的问题，在这项工作中没有解决。 然而，我们强调，由于存在许多局部最优[15]，在通信约束下我们的问题设置仍然是非常重要的。

## EASGD update rule
