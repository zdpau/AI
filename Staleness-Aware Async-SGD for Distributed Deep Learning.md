Staleness-Aware Async-SGD for Distributed Deep Learning

https://www.ijcai.org/Proceedings/16/Papers/335.pdf

## abstract
（Deep neural networks have been shown to achieve state-of-the-art performance in several machine learning tasks. Stochastic Gradient Descent (SGD) is the preferred optimization algorithm for training these networks and asynchronous SGD (ASGD) has been widely adopted for accelerating the training of large-scale deep networks in a distributed computing environment. However, in practice it is quite challenging to tune the training hyperparameters(such as learning rate) when using ASGD so as achieve convergence and linear speedup, since the stability of the optimization algorithm is strongly influenced by the asynchronous nature of parameter updates. In this paper, we propose a variant of the ASGD algorithm in which the learning rate is modulated according to the gradient staleness and provide theoretical guarantees for convergence of this algorithm. Experimental verification is performed on commonly-used image classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and the conventional ASGD algorithm.）

深度神经网络已被证明可以在多个机器学习任务中实现最先进的性能。随机梯度下降（SGD）是用于训练这些网络的优选优化算法，并且异步SGD（ASGD）已被广泛用于加速分布式计算环境中的大规模深度网络的训练。然而，在实践中，在使用ASGD时调整训练超参数（例如学习速率）以实现收敛和线性加速是非常具有挑战性的，因为优化算法的稳定性受到参数更新的异步性质的强烈影响。在本文中，我们提出了ASGD算法的一种变体，其中学习速率根据梯度过时而被调制，并为该算法的收敛提供理论保证。与SSGD（同步SGD）和传统ASGD算法相比，在常用的图像分类基准：CIFAR10和Imagenet上进行了实验验证，以证明所提方法的优越性。

## Introduction
Large-scale deep neural networks training is often constrained by the available computational resources, motivating the development of computing infrastructure designed specifically for accelerating this workload.This includes distributing the training across several commodity CPUs ([Dean et al., 2012],[Chilimbi et al., 2014]),or using heterogeneous computing platforms containing multiple GPUs per computing node ([Seide et al., 2014],[Wu et al., 2015],[Strom, 2015]), or using a CPU-based HPC cluster ([Gupta et al., 2015]).

大规模深度神经网络训练通常受到可用计算资源的限制，促使专门为加速这种工作量而设计的计算基础设施的发展。这包括将培训分布在多个商品CPU上（[Dean et al。，2012]，[Chilimbi et al。，2014]），或者使用每个计算节点包含多个GPU的异构计算平台（[Seide et al。，2014]， [Wu et al。，2015]，[Strom，2015]），或使用基于CPU的HPC集群（[Gupta et al。，2015]）。

Synchronous SGD (SSGD) is the most straightforward distributed implementation of SGD in which the master simply splits the workload amongst the workers at every iteration. Through the use of barrier synchronization, the master ensures that the workers perform gradient computation using the identical set of model parameters. The workers are forced to wait for the slowest one at the end of every iteration. This synchronization cost deteriorates the scalability and runtime performance of the SSGD algorithm. Asynchronous SGD(ASGD) overcomes this drawback by removing any explicit synchronization amongst the workers. However, permitting this asynchronous behavior inevitably adds “staleness” to the system wherein some of the workers compute gradients using model parameters that may be several gradient steps behind the most updated set of model parameters. Thus when fixing the number of iterations, ASGD-trained model tends to be much worse than SSGD-trained model. Further, there is no known principled approach for tuning learning rate in ASGD to effectively counter the effect of stale gradient updates.

同步SGD（SSGD）是SGD最直接的分布式实现，其中主服务器在每次迭代时简单地在工作者之间拆分工作负载。通过使用屏障同步，主控器确保工作人员使用相同的模型参数集执行梯度计算。在每次迭代结束时，工人被迫等待最慢的工人。此同步成本会降低SSGD算法的可伸缩性和运行时性能。异步SGD（ASGD）通过消除工作者之间的任何显式同步来克服这个缺点。然而，允许这种异步行为不可避免地给系统增加了“陈旧性”，其中一些工作人员使用模型参数来计算梯度，该模型参数可以是最新更新的模型参数组之后的几个梯度步骤。因此，在确定迭代次数时，ASGD训练的模型往往比SSGD训练的模型差得多。此外，没有已知的原理方法来调整ASGD中的学习速率以有效地抵消陈旧梯度更新的影响。

Prior theoretical work by [Tsitsiklis et al., 1986] and [Agarwal and Duchi, 2011] [Liu et al., 2013] and recent work by [Liu and Wright, 2015], [Lian et al., 2015], [Zhang et al., 2015] provide theoretical guarantees for convergence of stochastic optimization algorithms in the presence of stale gradient updates for convex optimization and nonconvex optimization, respectively. We find that adopting the approach of scale-out deep learning using ASGD gives rise to complex interdependencies between the training algorithm’s hyperparameters(such as learning rate, mini-batch size) and the distributed implementation’s design choices (such as synchronization protocol, number of learners), ultimately impacting the neural network’s accuracy and the runtime performance. In practice, achieving good model accuracy through distributed training requires a careful selection of the training hyperparameters and much of the prior work cited above lacks enough useful insight to help guide this selection process.

[Tsitsiklis et al。，1986]和[Agarwal and Duchi，2011] [Liu et al。，2013]的先前理论工作和[Liu and Wright，2015]，[Lian et al。，2015]，[ Zhang et al。，2015]分别为存在凸优化和非凸优化的陈旧梯度更新提供了随机优化算法收敛的理论保证。我们发现采用ASGD的横向扩展深度学习方法会导致训练算法的超参数（如学习速率，小批量）和分布式实现的设计选择（如同步协议，学习者数量）之间复杂的相互依赖性。 ），最终影响神经网络的准确性和运行时性能。在实践中，通过分布式培训实现良好的模型准确性需要仔细选择培训超参数，并且上面引用的大部分先前工作缺乏足够的有用见解来帮助指导此选择过程。

The work presented in this paper intends to fill this void by undertaking a study of the interplay between the different design parameters encountered during distributed training of deep neural networks. In particular, we focus our attention on understanding the effect of stale gradient updates during distributed training and developing principled approaches for mitigating these effects. To this end, we introduce a variant of the ASGD algorithm in which we keep track of the staleness associated with each gradient computation and adjust the learning rate on a per-gradient basis by simply dividing the learning rate by the staleness value. The implementation of this algorithm on a CPU-based HPC cluster with fast interconnect is shown to achieve a tight bound on the gradient staleness. We experimentally demonstrate the effectiveness of the proposed staleness-dependent learning rate scheme using commonly-used image classification benchmarks: CIFAR10 and Imagenet and show that this simple, yet effective technique is necessary for achieving good model accuracy during distributed training. Further, we build on the theoretical framework of [Lian et al., 2015] and prove that the convergence rate of the staleness-aware ASGD algorithm is consistent with SGD:O(1/根号t),where T is the number of gradient update steps.

本文提出的工作旨在通过研究深度神经网络分布式训练中遇到的不同设计参数之间的相互作用来填补这一空白。特别是，我们将注意力集中在了解分布式培训期间陈旧梯度更新的影响，并制定减轻这些影响的原则方法。为此，我们引入了ASGD算法的变体，其中我们跟踪与每个梯度计算相关的陈旧性，并通过简单地将学习率除以过时值来基于每个梯度调整学习率。该算法在具有快速互连的基于CPU的HPC集群上的实现被证明可以实现梯度过时的紧密限制。我们通过使用常用的图像分类基准：CIFAR10和Imagenet实验证明了所提出的与陈旧性相关的学习率方案的有效性，并表明这种简单而有效的技术对于在分布式训练期间实现良好的模型准确性是必要的。此外，我们建立在[Lian et al。，2015]的理论框架上，并证明了陈旧性感知ASGD算法的收敛速度与SGD一致：O(1/根号t),其中T是梯度更新步骤的数量。

Previously, [Ho et al., 2013] presented a parameter server based distributed learning system where the staleness in parameter updates is bounded by forcing faster workers to wait for their slower counterparts. Perhaps the most closely related prior work is that of [Chan and Lane, 2014] which presented a multi-GPU system for distributed training of speech CNNs and acknowledge the need to modulate the learning rate in the presence of stale gradients. The authors proposed an exponential penalty for stale gradients and show results for up to 5 learners, without providing any theoretical guarantee of the convergence rate. However, in larger-scale distributed systems, the gradient staleness can assume values up to a few hundreds ([Dean et al., 2012]) and the exponential penalty may reduce the learning rate to an arbitrarily small value, potentially slowing down the convergence. In contrast, in this paper, we formally prove our proposed ASGD algorithm to converge as fast as SSGD. Further, our implementation achieves near-linear speedup while maintaining the optimal model accuracy. We demonstrate this on widely used image classification benchmarks.

以前，[Ho et al。，2013]提出了一种基于参数服务器的分布式学习系统，其中参数更新中的陈旧性受到迫使更快的工人等待其较慢的对应物的限制。也许最密切相关的先前工作是[Chan and Lane，2014]，其提出了用于语音CNN的分布式训练的多GPU系统，并且承认在存在陈旧渐变的情况下调节学习速率的需要。作者提出了对陈旧梯度的指数惩罚，并显示多达5个学习者的结果，而没有提供任何理论上的收敛率保证。然而，在较大规模的分布式系统中，梯度过时可以假设值高达数百（[Dean et al。，2012]），指数惩罚可能会将学习率降低到任意小的值，从而可能减慢收敛速度。相比之下，在本文中，我们正式证明了我们提出的ASGD算法收敛速度与SSGD一样快。此外，我们的实现实现了接近线性的加速，同时保持了最佳的模型精度。我们在广泛使用的图像分类基准上证明了这一点。

## 2 System architecture
In this section we present an overview of our distributed deep learning system and describe the synchronization protocol design. In particular, we introduce the n-softsync protocol which enables a fine-grained control over the upper bound on the gradient staleness in the system. For a complete comparison, we also implemented the Hardsync protocol (aka SSGD) for model accuracy baseline since it generates the most accurate model (when fixing the number of training epochs), albeit at the cost of poor runtime performance.

在本节中，我们将概述分布式深度学习系统并描述同步协议设计。 特别是，我们引入了n-softsync协议，该协议能够对系统中梯度过时的上限进行细粒度控制。 为了进行完整的比较，我们还为模型精度基线实现了Hardsync协议（又名SSGD），因为它生成了最准确的模型（在确定训练时期的数量时），尽管以运行时性能较差为代价。
### 2.1 Architecture Overview
We implement a parameter server based distributed learning system, which is a generalization of Downpour SGD in [Dean et al., 2012], to evaluate the effectiveness of our proposed staleness-dependent learning rate modulation technique. Throughout the paper, we use the following definitions:

我们实现了一个基于参数服务器的分布式学习系统，它是[Dean et al。，2012]中Downpour SGD的推广，用于评估我们提出的与陈旧度相关的学习速率调制技术的有效性。 在整篇论文中，我们使用以下定义：





