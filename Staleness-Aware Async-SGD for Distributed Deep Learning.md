Staleness-Aware Async-SGD for Distributed Deep Learning

https://www.ijcai.org/Proceedings/16/Papers/335.pdf

## abstract
（Deep neural networks have been shown to achieve state-of-the-art performance in several machine learning tasks. Stochastic Gradient Descent (SGD) is the preferred optimization algorithm for training these networks and asynchronous SGD (ASGD) has been widely adopted for accelerating the training of large-scale deep networks in a distributed computing environment. However, in practice it is quite challenging to tune the training hyperparameters(such as learning rate) when using ASGD so as achieve convergence and linear speedup, since the stability of the optimization algorithm is strongly influenced by the asynchronous nature of parameter updates. In this paper, we propose a variant of the ASGD algorithm in which the learning rate is modulated according to the gradient staleness and provide theoretical guarantees for convergence of this algorithm. Experimental verification is performed on commonly-used image classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and the conventional ASGD algorithm.）

深度神经网络已被证明可以在多个机器学习任务中实现最先进的性能。随机梯度下降（SGD）是用于训练这些网络的优选优化算法，并且异步SGD（ASGD）已被广泛用于加速分布式计算环境中的大规模深度网络的训练。然而，在实践中，在使用ASGD时调整训练超参数（例如学习速率）以实现收敛和线性加速是非常具有挑战性的，因为优化算法的稳定性受到参数更新的异步性质的强烈影响。在本文中，我们提出了ASGD算法的一种变体，其中学习速率根据梯度过时而被调制，并为该算法的收敛提供理论保证。与SSGD（同步SGD）和传统ASGD算法相比，在常用的图像分类基准：CIFAR10和Imagenet上进行了实验验证，以证明所提方法的优越性。

## Introduction
Large-scale deep neural networks training is often constrained by the available computational resources, motivating the development of computing infrastructure designed specifically for accelerating this workload.This includes distributing the training across several commodity CPUs ([Dean et al., 2012],[Chilimbi et al., 2014]),or using heterogeneous computing platforms containing multiple GPUs per computing node ([Seide et al., 2014],[Wu et al., 2015],[Strom, 2015]), or using a CPU-based HPC cluster ([Gupta et al., 2015]).

大规模深度神经网络训练通常受到可用计算资源的限制，促使专门为加速这种工作量而设计的计算基础设施的发展。这包括将培训分布在多个商品CPU上（[Dean et al。，2012]，[Chilimbi et al。，2014]），或者使用每个计算节点包含多个GPU的异构计算平台（[Seide et al。，2014]， [Wu et al。，2015]，[Strom，2015]），或使用基于CPU的HPC集群（[Gupta et al。，2015]）。

Synchronous SGD (SSGD) is the most straightforward distributed implementation of SGD in which the master simply splits the workload amongst the workers at every iteration. Through the use of barrier synchronization, the master ensures that the workers perform gradient computation using the identical set of model parameters. The workers are forced to wait for the slowest one at the end of every iteration. This synchronization cost deteriorates the scalability and runtime performance of the SSGD algorithm. Asynchronous SGD(ASGD) overcomes this drawback by removing any explicit synchronization amongst the workers. However, permitting this asynchronous behavior inevitably adds “staleness” to the system wherein some of the workers compute gradients using model parameters that may be several gradient steps behind the most updated set of model parameters. Thus when fixing the number of iterations, ASGD-trained model tends to be much worse than SSGD-trained model. Further, there is no known principled approach for tuning learning rate in ASGD to effectively counter the effect of stale gradient updates.

同步SGD（SSGD）是SGD最直接的分布式实现，其中主服务器在每次迭代时简单地在工作者之间拆分工作负载。通过使用屏障同步，主控器确保工作人员使用相同的模型参数集执行梯度计算。在每次迭代结束时，工人被迫等待最慢的工人。此同步成本会降低SSGD算法的可伸缩性和运行时性能。异步SGD（ASGD）通过消除工作者之间的任何显式同步来克服这个缺点。然而，允许这种异步行为不可避免地给系统增加了“陈旧性”，其中一些工作人员使用模型参数来计算梯度，该模型参数可以是最新更新的模型参数组之后的几个梯度步骤。因此，在确定迭代次数时，ASGD训练的模型往往比SSGD训练的模型差得多。此外，没有已知的原理方法来调整ASGD中的学习速率以有效地抵消陈旧梯度更新的影响。

Prior theoretical work by [Tsitsiklis et al., 1986] and [Agarwal and Duchi, 2011] [Liu et al., 2013] and recent work by [Liu and Wright, 2015], [Lian et al., 2015], [Zhang et al., 2015] provide theoretical guarantees for convergence of stochastic optimization algorithms in the presence of stale gradient updates for convex optimization and nonconvex optimization, respectively. We find that adopting the approach of scale-out deep learning using ASGD gives rise to complex interdependencies between the training algorithm’s hyperparameters(such as learning rate, mini-batch size) and the distributed implementation’s design choices (such as synchronization protocol, number of learners), ultimately impacting the neural network’s accuracy and the runtime performance. In practice, achieving good model accuracy through distributed training requires a careful selection of the training hyperparameters and much of the prior work cited above lacks enough useful insight to help guide this selection process.

[Tsitsiklis et al。，1986]和[Agarwal and Duchi，2011] [Liu et al。，2013]的先前理论工作和[Liu and Wright，2015]，[Lian et al。，2015]，[ Zhang et al。，2015]分别为存在凸优化和非凸优化的陈旧梯度更新提供了随机优化算法收敛的理论保证。我们发现采用ASGD的横向扩展深度学习方法会导致训练算法的超参数（如学习速率，小批量）和分布式实现的设计选择（如同步协议，学习者数量）之间复杂的相互依赖性。 ），最终影响神经网络的准确性和运行时性能。在实践中，通过分布式培训实现良好的模型准确性需要仔细选择培训超参数，并且上面引用的大部分先前工作缺乏足够的有用见解来帮助指导此选择过程。

