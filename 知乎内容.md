## 从零开始：教你如何训练神经网络：https://zhuanlan.zhihu.com/p/31953880
从最基础的感知机讲起，讲了神经网络的基础知识。用于普及神经网络知识。
## 无需深度学习框架，如何从零开始用Python构建神经网络：https://zhuanlan.zhihu.com/p/37340090

## 为什么我们更宠爱“随机”梯度下降？：https://zhuanlan.zhihu.com/p/28060786
介绍了SGD和GD的区别，并举例了SGD为什么前期收敛快。
>*结论：1,如果样本数量大，那么 SGD的Computational Complexity （可以大概理解成，达成目标需要计算 Gradient 的次数）依然有优势。
2, 相对于非随机算法，SGD 能更有效的利用信息，特别是信息比较冗余的时候.
3,相对于非随机算法， SGD 在前期迭代效果卓越.

如果想随机梯度下降收敛的比较好，我知道的方法有，一是采取递减的 stepsize，原理是什么我还没搞清楚，二就是 Variance reduction 的一些方法吧，比如采用更多的样本算梯度，或者用SAGA, SVRG这种方法吧。（只是作者个人猜测）

最后的评论有点干货。
