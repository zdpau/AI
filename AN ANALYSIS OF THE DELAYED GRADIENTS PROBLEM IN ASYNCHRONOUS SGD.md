# AN ANALYSIS OF THE DELAYED GRADIENTS PROBLEM IN ASYNCHRONOUS SGD

## Abstract
### 梯度下降可以通过数据并行或模型并行在多个worker之间有效分配。 所有方法的共同目标是最大限度地减少工人的闲置时间。 参数同步（例如SGD中的每个小批次之后）需要参数服务器在应用更新之前等待最慢的工作人员回复。有一种完全异步的方法（Dean等，2012），最初称为Downpour SGD，它通过允许将陈旧参数上计算出的梯度发送到参数服务器来最小化工人空闲时间。 在实践中，异步SGD的直接使用会导致在从渐变梯度（during training from stale gradient）（称为“延迟梯度问题”）训练期间增加噪声，这会不准确地降低测试精度。延迟补偿，如Zheng等人详述的那样。 （2016a）以及各种暖启动计划可以帮助融合。在本文中，我们详细分析了在超参数选择的大范围扫描下由于延迟梯度引起的ASGD故障模式。 使用卷积模型，我们发现学习速率和批量选择是延迟梯度是否显著降低测试精度的主要因素。 仔细选择学习速率和批量大小，或使用自适应学习速率方法，可以有效地将延迟梯度问题减至最少（n = 257）。

## Introduction
### 神经网络训练已经通过模型并行模型（其中模型分散在不同的工作人员中）和数据并行性（其中训练数据被分解或分配给工作人员副本）扩展到许多工作人员。 分布式方法在模型参数如何同步方面有所不同，但最快（每单位时间计算的梯度）方法是完全异步的，并允许从陈旧参数计算梯度更新（Dean等，2012）。异步优化方法在共享集群中具有优势，其中单个工作人员可能会遇到来自并行作业，不同网络条件或异构硬件的缓慢下降。 陈等人（2017）添加额外的备份工作人员，以便运行同步SGD的参数服务器可以在最慢的工作人员回复之前继续下一批。
### 然而，由于梯度的延迟应用，ASGD遭受了准确度降低，为此已经提出了各种延迟补偿方法：
### •Dean等人。 （2012）发现Adagrad自适应学习率优化大大提高了Downpour SGD的稳健性。
### •陈等人。 （2017）发现，在前三个时期逐渐引入工人对于高延迟值下的稳定性非常重要。 作者还剪裁了梯度 for ASGD。
### •郑等人。 （2016b）将梯度函数的泰勒展开中的一阶项添加到延迟梯度提交。
### 为了增加同步训练中的每个worker的工作量，Goyal等人 （2017年）使用大型小批量（8k图像）分布在整个集群中，并且按比例大的学习率。 即使在ASGD中，mini-batches不在工人之间分配，大型小批量和学习率也会降低工人与服务器之间的通信频率。我们研究了在延迟梯度存在下，批量大小和学习速率对收敛的影响，以便更好地推荐ASGD的精确用例。

## method
### 在我们的实验中，我们训练了一个Lenet-5模型，用于MNIST数字识别任务超过30个时期（epochs），使用SGD(m = 0.9)。为了模拟异步SGD中的延迟梯度提交，我们创建了一个围绕同步PyTrar优化器的包装器，它在缓冲器中存储梯度并在恒定延迟之后再应用它们。
###
