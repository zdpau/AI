http://lib.csdn.net/article/computervison/50556
https://blog.csdn.net/maxiao1204/article/details/65653781
https://blog.csdn.net/antony1776/article/details/74637248 这篇把整个神经网络梳理了一遍，需要再看

过去几年，深度学习在解决诸如视觉识别、语音识别和自然语言处理等很多问题方面都表现出色。在不同类型的神经网络当中，卷积神经网络是得到最深入研究的。早期由于缺乏训练数据和计算能力，要在不产生过拟合的情况下训练高性能卷积神经网络是很困难的。标记数据和近来GPU的发展，使得卷积神经网络研究涌现并取得一流结果。

ANN（人工神经网络）历史：
>* 1943年，神经科学家和控制论专家Warren McCulloch和逻辑学家Walter Pitts基于数学和阈值逻辑算法创造了一种神经网络计算模型；
>* 1957年，心理学家Frank Rosenblatt创造了模式识别算法感知机，用简单的加减算法实现了两层的计算机学习网络；
>* 1974年，Paul Werbos在博士论文中提出了用误差反向传导来训练人工神经网络有效解决了异或回路问题，使得训练多层神经网络称为可能；
>* 1985年，Rumelhart和McClelland提出了BP网络误差反向传播学习算法；
>* 1998年，以Yann Lecun为首的研究人员实现了一个七层的卷积神经网络LeNet-5识别手写数字；
>* 2006年，以Geoffrey Hinton为代表的加拿大高等研究院附属机构的研究人员开始将人工神经网络/联结主义重新包装为深度学习并进行推广；
>* 2009-2012年，瑞士人工智能实验室IDSIA发展了递归神经网络和深前馈神经网络；
>* 2012年，Geoffrey Hinton组的研究人员在ImageNet2012上夺冠，他们的图像分类效果远远超过了第二名，深度学习热潮由此开始一直持续到现在

备注：ImageNet 是一个计算机视觉系统识别项目， 是目前世界上图像识别最大的数据库。

## 机器学习的算法，主要可分为以下三类：
>* 信息论：计算数据中属性的信息量，以信息增量作为决策依据；决策树算法为代表，如ID3，IBLE算法；（信息量是概率倒数的对数） 
>* 集合论：聚类分析，相似度，距离算法； 
>* 仿生物技术：把生物体的运转过程转换成数学模型，再用数学模型去解决现实世界的非生物问题，如神经网络、遗传算法！

每个权重的梯度都等于与其相连的前一层节点的输出乘以与其相连的后一层的反向传播的输出。

## 经典的神经网络模型：
>* Lenet，1986年
>* Alexnet，2012年
>* GoogleNet，2014年
>* VGG，2014年
>* Deep Residual Learning，2015年
## 1 Lenet-5

### 20世纪 90 年代，LeCun et al. [3] 等人发表论文，确立了CNN的现代结构，后来又对其进行完善。他们设计了一种多层的人工神经网络，取名叫做LeNet-5，可以对手写数字做分类。和其他神经网络一样， LeNet-5 也能使用 backpropagation 算法训练。 

### LeNet5 的架构基于这样的观点：（尤其是）图像的特征分布在整张图像上，以及带有可学习参数的卷积是一种用少量参数在多个位置上提取相似特征的有效方式。LeNet5特征能够总结为如下几点：1）卷积神经网络使用三个层作为一个系列： 卷积，池化，非线性 2） 使用卷积提取空间特征 3）使用映射到空间均值下采样（subsample） 4）双曲线（tanh）或S型（sigmoid）形式的非线性 5）多层神经网络（MLP）作为最后的分类器 6）层与层之间的稀疏连接矩阵避免大的计算成本　

## 2 AlexNet

### 2006年起，人们设计了很多方法，想要克服难以训练深度CNN的困难。其中，最著名的是2012年，Hinton的学生Alex Krizhevsky提出了深度卷积神经网络模型AlexNet，它可以算是LeNet的一种更深更宽的版本。AlexNet中包含了几个比较新的技术点，也首次在CNN中成功应用了ReLU、Dropout和LRN等Trick。同时AlexNet也使用了GPU进行运算加速，作者开源了他们在GPU上训练卷积神经网络的CUDA代码。AlexNet包含了6亿3000万个连接，6000万个参数和65万个神经元，拥有5个卷积层，其中3个卷积层后面连接了最大池化层，最后还有3个全连接层。

### AlexNet主要使用到的新技术点如下。

#### （1）成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。

#### （2）训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。

#### （3）在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。

#### （4）提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

#### （5）使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。AlexNet使用了两块GTX 580 GPU进行训练，单个GTX 580只有3GB显存，这限制了可训练的网络的最大规模。因此作者将AlexNet分布在两个GPU上，在每个GPU的显存中储存一半的神经元的参数。因为GPU之间通信方便，可以互相访问显存，而不需要通过主机内存，所以同时使用多块GPU也是非常高效的。同时，AlexNet的设计让GPU之间的通信只在网络的某些层进行，控制了通信的性能损耗。 

#### （6）数据增强，随机地从256´256的原始图像中截取224´224大小的区域（以及水平翻转的镜像），相当于增加了(256-224)2´2=2048倍的数据量。如果没有数据增强，仅靠原始的数据量，参数众多的CNN会陷入过拟合中，使用了数据增强后可以大大减轻过拟合，提升泛化能力。进行预测时，则是取图片的四个角加中间共5个位置，并进行左右翻转，一共获得10张图片，对他们进行预测并对10次结果求均值。同时，AlexNet论文中提到了会对图像的RGB数据进行PCA处理，并对主成分做一个标准差为0.1的高斯扰动，增加一些噪声，这个Trick可以让错误率再下降1%。

#### 具体内容见第二个链接

### 3 Overfeat

#### 2013 年的 12 月，纽约大学的 Yann LeCun 实验室提出了 AlexNet 的衍生——Overfeat（参见：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks）。这篇文章也提出了学习边界框（learning bounding box），并导致之后出现了很多研究这同一主题的论文。

（具体内容见第二个链接）AlexNet 取得成功后，研究人员又提出了其他的完善方法，其中最著名的要数 ZFNet [7], VGGNet [8], GoogleNet [9] 和 ResNet [10] 这四种。从结构看，CNN 发展的一个方向就是层数变得更多，ILSVRC 2015 冠军 ResNet 是 AlexNet 的20 多倍，是 VGGNet 的8 倍多。






